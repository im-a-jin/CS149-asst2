\documentclass[11pt]{article}

\input{$HOME/imajin/preamble.tex}

\begin{document}

\lstset{language=C++,basicstyle=\footnotesize\ttfamily,breaklines=true}

\fancyhf{}
\fancyhead[R]{Derek Chong (derekch), Matthew Jin (mjin2002)}
\setlength{\headheight}{14pt}
\pagestyle{fancy}

\centerline{\Large CS 149: Programming Assignment 2}
\centerline{Autumn 2024-25}

\section{Task System Implementation}

Our task system implementation uses a thread pool to store the active threads.
Each thread is pointed to a \ttt{runTaskWrapper} function that takes in the
thread arguments listed below:
\begin{lstlisting}
struct TaskArgsA3 {
    int thread_id;                  // thread id
    int num_threads;                // total number of threads
    bool *done;                     // true if destructor called, false otherwise
    int *work_queue;                // task_id counter
    pthread_mutex_t *mutex_lock;    // shared mutex lock
    pthread_cond_t *wake;           // thread sleep/done condition variable
    pthread_cond_t *all_done;       // thread completed 
    std::atomic<int> *tasks_done;   // counter for tasks done
    IRunnable **runnable;           // pointer to runnable object
    int *num_total_tasks;           // total number of tasks
};
\end{lstlisting}

For part A, we assign tasks to the workers dynamically using a \ttt{work\_queue}
counter to keep track of the next available \ttt{task\_id}. Each task grabs the
next \ttt{task\_id} under the \ttt{mutex\_lock} and increments the counter
before releasing the lock and running the task. Once a thread is finished
running a task, it will increment the atomic \ttt{tasks\_done} counter by one.
We use \ttt{std::memory\_order\_relaxed} for its speed since the loads and
stores across different threads can be arbitrarily ordered so long as each
thread checks the stopping condition once after their last store to the counter.

\smallskip
At program start, the threads are spawned in the constructor, and the main
thread broadcasts the \ttt{wake} condition variable after setting
\ttt{work\_queue} to 0 and \ttt{runnable} and \ttt{num\_total\_tasks}
correspondingly. Then, the main thread sleeps on \ttt{all\_done} in a while loop
that checks if all the tasks are done. At this point, the spawned threads is either
waiting on the \ttt{mutex\_lock} or has fallen through to the point where it is
sleeping on \ttt{wake}. Once the broadcast is received, all threads are active
and start grabbing (incrementing) tasks off the work queue.

\smallskip
Once a thread is no longer able to grab any new tasks, that is
$\ttt{work\_queue} > \ttt{num\_total\_tasks}$, it signals to the main thread
that it is done via the \ttt{all\_done} condition variable before sleeping on
the \ttt{wake} condition variable with the \ttt{mutex\_lock}. The main thread
checks if \tit{all} tasks are done, since some threads may still be running
their tasks. If this is not the case, it goes back to sleep. If it is, it resets
the run variables before returning. Now, when a second call to run happens, all
individual threads will be waiting on \ttt{wake}.

\bigskip
For part B, to ensure correct execution of task graphs, we track dependencies by
representing task runs as a directed acyclic graph, and only permitting runs to
start when their dependencies have all finished running. More specifically, we
use a reference counting scheme similar to Kahn's algorithm. Each run maintains: 

\begin{lstlisting}
struct TaskArgsB {
    int thread_id;                              // thread id
    int num_threads;                            // total number of threads
    bool *done;                                 // true if destructor called
    std::queue<TaskGraphNode *> *work_queue;    // work queue
    std::vector<TaskGraphNode *> *task_graph;   // task graph
    pthread_mutex_t *wq_lock;                   // work queue lock
    pthread_mutex_t *tg_lock;                   // task graph lock
    pthread_cond_t *wake;                       // thread sleep condition variable
    pthread_cond_t *all_done;                   // thread completed (queue empty)
};

class TaskGraphNode {
    public:
        int node_id;                            // bulk task (node) ID
        int task_id;                            // task ID counter
        std::atomic<int> tasks_done;            // tasks done counter
        IRunnable *runnable;                    // task runnable
        int num_total_tasks;                    // number of total tasks
        int num_deps;                           // number of incoming deps
        std::vector<TaskID> deps_out;           // list of bulk tasks (nodes)
                                                // that depend on this task

        TaskGraphNode(int node_id, IRunnable *runnable, int num_total_tasks) {
            this->node_id = node_id;
            this->task_id = 0;
            this->tasks_done = 0;
            this->runnable = runnable;
            this->num_total_tasks = num_total_tasks;
            this->num_deps = 0;
        }
};
\end{lstlisting}

Both the work queue and the task graph are data structures that store pointers
to \ttt{TaskGraphNode} objects, and each has their own lock. These objects are
allocated on the heap when \ttt{run} is called and are deleted by the
destructor. Putting at a fixed location on the heap instead of in a data
structure avoids any issues that come from pointers changing when a vector is
resized and other related problems.

\smallskip
The task graph maintains a list of pointers to \ttt{TaskGraphNode}s where the
index is that node's bulk task ID (which we call node ID). At each run call, the
\ttt{TaskGraphNode} is populated with the relevent parameters. We also iterate
through the existing task graph and mark any completed dependencies. For
dependencies that haven't finished, the node's ID is added to their
dependencies' \ttt{deps\_out} which will be updated upon bulk task completion.

\smallskip
The work queue maintains pointers to bulk tasks/nodes which are currently
permitted to start. Threads will greedily grab work off the front of the queue,
popping pointers when the task is finished and sleeping when there is no work to
be done. To keep this list updated, any time a run finishes all of its tasks we
iterate through all runs which depend on it, and decrement each of their
dependency counters. If any runs' counter is now zero, we add it to the ready
list. This approach has several nice properties:

\begin{itemize}
    \item Simple data structure: Both queue and graph data structures only need
      to store pointers to node locations which minimizes the overhead from
      copying elements.
    \item Minimal compute overhead: We never need to sweep through all runs.
      Adding runs only touches previous runs' dependency lists and updates the
      back of the vector. Tracking completion does not require graph structure
      updates: it only touches a single counter per dependency, and just once
      each per bulk run.
    \item Thread-safety: This approach enables a very small critical section.
      While both the work queue and task graph can access the same
      \ttt{TaskGraphNode}, the specific memory locations/variables they access
      are disjoint. This allows the two data structures to be locked separately,
      minimizing locking overhead. We also make use of local storage to avoid
      nesting locks.
\end{itemize}

\section{Comparing Implementation Performance}

One instance where the serial implementation performs better than any of the
threaded implementations are for the tests that are very computationally light
such as \ttt{super\_super\_light} and \ttt{super\_light}. In these cases, the
overhead for creating threads and have them destroyed/spinning/sleeping becomes
a non-insignificant part of the runtime.

\medskip
The spawn-every-launch thread implementation performs as well as the more
advanced thread pool implementations for tests such as
\ttt{math\_operations\_in\_tight\_for\_loop\_reduction\_tree} and
\ttt{recursive\_fibonacci}. These tasks are computationally expensive, so the
overhead from thread operations has less impact on the overall runtime. In
addition, these tasks have fewer bulk task launches (\ttt{recursive\_fibonacci}
has 30, \ttt{math\_operations\_in\_tight\_for\_loop\_reduction\_tree} has 32)
which means that fewer threads are created and destroyed in the
spawn-every-launch implementation.

\smallskip
On the other hand, spawn-every-launch performs worse for
\ttt{math\_operations\_in\_tight\_for\_loop} and \ttt{ping\_pong\_equal} which
have 2000 and 400 bulk task launches respectively. Since there are more bulk
task launches, spawn-every-launch will have to create and destroy more threads,
leading to more overhead and higher runtime.

\section{Custom Testing}

In our initial implementation of Part B, we started with the simplest possible
parallelized solution, which entailed using a global lock for all graph
operations. We observed that this resulted in surprising performance dynamics:
the sync versions of existing tests were running several times faster than the
async versions, despite both calling the same underlying functions and sync
calling \verb|blockuntilEmpty()| after every run.

\medskip
We created a set of tests to help characterize the source of this
underperformance. We started by creating a very general test function which
enabled us to construct many highly varied tests, providing granular control
over the number of total elements to compute, the number of bulk tasks to break
this into, whether runs were sync or async, and whether runs were dependent on
one another. We used \verb|SimpleMultiplyTask| as the underlying task.

\medskip
We then wrote multiple test endpoints which each called this function, varying
function arguments across every dimension in order to isolate their effects on
performance. In doing so, we ruled out dependency management as the source of
the performance gap. Our tests with many low-compute runs validated that
parallelization improved performance even with a global lock: async tests
exhibited 20-30x speedups over equivalent sync tests. Finally, we identified the
likely culprit by observing that tests with linear dependencies enabled were
counterintuitively finishing several times faster than those without
dependencies. This pointed to lock contention as the source of underperformance,
most likely between \verb|runAsyncWithDeps()| and graph updates related to task
completion. (In the process of testing, we also identified and fixed an edge
case correctness bug).

\medskip
This realization led us to change our implementation to utilize two locks: one
for the work queue and separate one for the task graph. With this method, one
thread can update the task graph upon task completion while allowing the
other threads to continue pulling work from the work queue if there is work
available.

\end{document}
